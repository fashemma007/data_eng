{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vietnamese-tuesday",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T00:51:44.521649Z",
     "start_time": "2021-08-17T00:51:44.514652Z"
    }
   },
   "source": [
    "# Data Wrangling with Spark\n",
    "\n",
    "This is the code used in the previous screencast. Run each code cell to understand what the code does and how it works.\n",
    "\n",
    "These first three cells import libraries, instantiate a SparkSession, and then read in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accredited-indication",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T13:14:57.460960Z",
     "start_time": "2021-08-17T13:14:57.431961Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alleged-explosion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T13:15:24.983727Z",
     "start_time": "2021-08-17T13:14:58.519940Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Wrangling Data\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "accredited-teddy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T13:46:26.832485Z",
     "start_time": "2021-08-17T13:31:05.874249Z"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o30.json.\n: org.apache.hadoop.net.ConnectTimeoutException: Call From My-Spectre-DESKTOP-QQK1V33/192.168.88.11 to ec2-34-218-86-174.us-west-2.compute.amazonaws.com:9000 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=ec2-34-218-86-174.us-west-2.compute.amazonaws.com/34.218.86.174:9000]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)\r\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:775)\r\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1457)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1367)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\r\n\tat com.sun.proxy.$Proxy18.getFileInfo(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:903)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\r\n\tat com.sun.proxy.$Proxy19.getFileInfo(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1665)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1582)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1579)\r\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1594)\r\n\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1700)\r\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:377)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\r\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:519)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=ec2-34-218-86-174.us-west-2.compute.amazonaws.com/34.218.86.174:9000]\r\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)\r\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:690)\r\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:794)\r\n\tat org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)\r\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1403)\r\n\t... 39 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-151696e66a95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"hdfs://ec2-34-218-86-174.us-west-2.compute.amazonaws.com:9000/sparkify/sparkify_log_small.json\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0muser_log\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\hp spectre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[1;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, allowNonNumericNumbers, modifiedBefore, modifiedAfter)\u001b[0m\n\u001b[0;32m    370\u001b[0m             \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 372\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    373\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp spectre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp spectre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp spectre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o30.json.\n: org.apache.hadoop.net.ConnectTimeoutException: Call From My-Spectre-DESKTOP-QQK1V33/192.168.88.11 to ec2-34-218-86-174.us-west-2.compute.amazonaws.com:9000 failed on socket timeout exception: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=ec2-34-218-86-174.us-west-2.compute.amazonaws.com/34.218.86.174:9000]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\r\n\tat java.lang.reflect.Constructor.newInstance(Unknown Source)\r\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:831)\r\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:775)\r\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1515)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1457)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1367)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\r\n\tat com.sun.proxy.$Proxy18.getFileInfo(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:903)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\r\n\tat com.sun.proxy.$Proxy19.getFileInfo(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1665)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1582)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1579)\r\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1594)\r\n\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1700)\r\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:377)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\r\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:519)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.hadoop.net.ConnectTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=ec2-34-218-86-174.us-west-2.compute.amazonaws.com/34.218.86.174:9000]\r\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)\r\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:690)\r\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:794)\r\n\tat org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:411)\r\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1572)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1403)\r\n\t... 39 more\r\n"
     ]
    }
   ],
   "source": [
    "path = \"hdfs://ec2-34-218-86-174.us-west-2.compute.amazonaws.com:9000/sparkify/sparkify_log_small.json\"\n",
    "user_log = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-envelope",
   "metadata": {},
   "source": [
    "# Data Exploration \n",
    "\n",
    "The next cells explore the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-gilbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "theoretical-foundation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T13:03:20.121370Z",
     "start_time": "2021-08-17T13:03:19.762313Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user_log' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e73e35fe97be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0muser_log\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'user_log' is not defined"
     ]
    }
   ],
   "source": [
    "user_log.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-present",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.describe(\"artist\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.describe(\"sessionId\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-atlas",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.select(\"page\").dropDuplicates().sort(\"page\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.select([\"userId\", \"firstname\", \"page\", \"song\"]).where(user_log.userId == \"1046\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-emperor",
   "metadata": {},
   "source": [
    "# Calculating Statistics by Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hour = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0). hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-england",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log = user_log.withColumn(\"hour\", get_hour(user_log.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_in_hour = user_log.filter(user_log.page == \"NextSong\").groupby(user_log.hour).count().orderBy(user_log.hour.cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_in_hour = user_log.filter(user_log.page == \"NextSong\").groupby(user_log.hour).count().orderBy(user_log.hour.cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_in_hour_pd = songs_in_hour.toPandas()\n",
    "songs_in_hour_pd.hour = pd.to_numeric(songs_in_hour_pd.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-purple",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(songs_in_hour_pd[\"hour\"], songs_in_hour_pd[\"count\"])\n",
    "plt.xlim(-1, 24);\n",
    "plt.ylim(0, 1.2 * max(songs_in_hour_pd[\"count\"]))\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Songs played\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-opera",
   "metadata": {},
   "source": [
    "# Drop Rows with Missing Values\n",
    "\n",
    "As you'll see, it turns out there are no missing values in the userID or session columns. But there are userID values that are empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_valid = user_log.dropna(how = \"any\", subset = [\"userId\", \"sessionId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-child",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_valid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.select(\"userId\").dropDuplicates().sort(\"userId\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_valid = user_log_valid.filter(user_log_valid[\"userId\"] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_valid.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-penetration",
   "metadata": {},
   "source": [
    "# Users Downgrade Their Accounts\n",
    "\n",
    "Find when users downgrade their accounts and then flag those log entries. Then use a window function and cumulative sum to distinguish each user's data as either pre or post downgrade events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-governor",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_valid.filter(\"page = 'Submit Downgrade'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.select([\"userId\", \"firstname\", \"page\", \"level\", \"song\"]).where(user_log.userId == \"1138\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_downgrade_event = udf(lambda x: 1 if x == \"Submit Downgrade\" else 0, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_valid = user_log_valid.withColumn(\"downgraded\", flag_downgrade_event(\"page\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowval = Window.partitionBy(\"userId\").orderBy(desc(\"ts\")).rangeBetween(Window.unboundedPreceding, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-eleven",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_valid = user_log_valid.withColumn(\"phase\", Fsum(\"downgraded\").over(windowval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_valid.select([\"userId\", \"firstname\", \"ts\", \"page\", \"level\", \"phase\"]).where(user_log.userId == \"1138\").sort(\"ts\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-yacht",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
